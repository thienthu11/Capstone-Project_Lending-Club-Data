
---
title: "Forecasting Credit Default"
author: "Thu Nguyen"
date: "9/30/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


**Prework: Load packages.** 
#Load needed packages <br>
```{r Load all needed packages}
library(kernlab)
library(xgboost)
library(zoo)
library(readr)
library(scales)
library(plyr)
library(gbm)
library(Metrics)
library(parsnip)
library(parttree)
library(rpart)
library(rpart.plot)
library(devtools)
library(ggplot2)
library(gridExtra)
library(repr)
library(dplyr)
library(caret)
library(randomForest)
library(MLmetrics)
library(tidyverse)
library(chron)
library(ROCR)
library(pROC)
library(quantmod)
library(stringr)
library(lattice)
library(e1071)
library(cluster)
library(data.table)
library(randomForest)
library(broom)
library(mapproj)
library(MASS)
library(lubridate)
```
#Loading Data
```{r}
loans <- read.csv("loans.csv")
```

### Part 1 (Exploring and Observing the data to understand it well)<br>

1/ Cleaning the dataset and choosing importance variables for my model later.

#Creating Meta Data_The meta data will give us more detail about the data such as which variables are present in loan data set and what is their type. We will need the meta data at a later stage so we assign it to a variable<br>
```{r}
meta_loans <- funModeling::df_status(loans, print_results = FALSE)
knitr::kable(meta_loans)
```
#Converting character variables to numeric<br>
```{r}
chr_to_num_vars <- c("annual_inc_joint", "mths_since_last_major_derog", "open_acc_6m", "open_il_12m", "open_il_24m", "mths_since_rcnt_il", "total_bal_il", "il_util", "open_rv_12m", "open_rv_24m", "max_bal_bc", "all_util", "total_rev_hi_lim", "total_cu_tl", "inq_last_12m", "dti_joint", "inq_fi", "tot_cur_bal", "tot_coll_amt")

loans <- loans %>% mutate_at(.funs = funs(as.numeric), .vars = chr_to_num_vars)
```
#Checking date variables<br>
```{r}
chr_to_date_vars <- c("issue_d", "last_pymnt_d", "last_credit_pull_d", "next_pymnt_d", "earliest_cr_line", "next_pymnt_d")

loans %>% select_(.dots = chr_to_date_vars) %>% str()

for (i in chr_to_date_vars){
  print(head(unique(loans[, i])))
  }
```
```{r}
detach(package:MASS)
meta_loans %>% select(variable, q_na) %>% filter(variable %in% chr_to_date_vars)
```
#Convert Date to correct data type<br>
```{r}
convert_date <- function(x){
  as.Date(paste0("01-", x), format = "%d-%b-%Y")
  } 

loans <-
  loans %>%
  mutate_at(.funs = funs(convert_date), .vars = chr_to_date_vars)

num_vars <- 
  loans %>% 
  sapply(is.numeric) %>% 
  which() %>% 
  names()

meta_loans %>%
  select(variable, p_zeros, p_na, unique) %>%
  filter_(~ variable %in% num_vars) %>%
  knitr::kable()
```
#Replacing the missing values in mths_since_last_delinq, mths_since_last_record, mths_since_last_major_derog with zeros and updating the meta data<br>
```{r}
na_to_zero_vars <- c("mths_since_last_delinq", "mths_since_last_record", "mths_since_last_major_derog")

loans <- loans %>% mutate_at(.vars = na_to_zero_vars, .funs = funs(replace(., is.na(.), 0)))

meta_loans <- funModeling::df_status(loans, print_results = FALSE)

meta_loans <- meta_loans %>% mutate(uniq_rat = unique / nrow(loans))
```

2/ Defining default loans

#Checking unique potential variables that may indicate a default or delay in payments: loan_status, delinq_2yrs, mths_since_last_delinq<br>
```{r}
default_vars <- c("loan_status", "delinq_2yrs", "mths_since_last_delinq")
purrr::map(.x = loans[, default_vars], .f = base::unique)
```
#Defining default loans as follow<br>
```{r}
defaulted <- c("Default", "Does not meet the credit policy. Status:Charged Off", "In Grace Period", "Late (16-30 days)", "Late (31-120 days)")

loans <- loans %>% mutate(default = ifelse(!(loan_status %in% defaulted), FALSE, TRUE))
```
#Compute the frequency of actual defaults in the training set. It is around 0.02286<br>
```{r}
detach(package:plyr)

loans %>% summarise(default_freq = sum(default / n()))

table(loans$default) / nrow(loans) 
```
#Selecting variables that fit for my modeling. I will remove variables that are key variable, have high NA ratio and have high amount of unique values<br>
```{r}
loans2 <- loans %>% select(loan_amnt, funded_amnt, funded_amnt_inv, loan_status, int_rate, grade, emp_length, home_ownership, annual_inc, term, purpose, total_acc, last_fico_range_high, last_fico_range_low, default)
```
#Creating train & test dataset<br>
```{r}
set.seed(6438)

train_index <- caret::createDataPartition(y = loans2$default, times = 1, p = .8, list = FALSE)
train <- loans2[train_index, ]
test <- loans2[-train_index, ]

rm(loans2)
```

###Part 2 (Analyzing relationship between variables which i think Lending Club based on to give approval decision)<br>

  2.1/ Explore income and loan amount

#Assign continous variables to a character vector and reshape data for plotting of distributions<br>
```{r}
library(scales)
income_vars <- c("annual_inc")
loan_amount_vars <- c("loan_amnt", "funded_amnt", "funded_amnt_inv")
```
#Reshape and plot the original data for specified variables in a tidyr-dplyr-ggplot pipe<br>
```{r}
train %>% select_(.dots = income_vars) %>% gather_("variable", "value", gather_cols = income_vars) %>% ggplot(aes(x = value)) + facet_wrap(~ variable, scales = "free_x", ncol = 3) + geom_histogram()
```
We can see that a lot of loans have corresponding annual income of zero and in general income seems low. Based on the meta data, joint income has a large number of NA values (i.e. cannot be displayed) and those few values that are present do not seem to have significant exposure. Most loan applications must have been submitted by single income borrowers.

#Loan_amnt Vs Funded_amnt Vs Funded_amnt_inv<br>
```{r}
train %>%
  select_(.dots = loan_amount_vars) %>%
  gather_("variable", "value", gather_cols = loan_amount_vars) %>%
  ggplot(aes(x = value)) +
  facet_wrap(~ variable, scales = "free_x", ncol = 3) +
  geom_histogram()
```
The loan amount distributions seems similar in shape suggesting not too much divergence between the loan amount applied for, the amount committed and the amount invested. It means that most likely qualified borrowers are going to get the loan they had applied for.

#Combine my selection with the meta information gathered in an earlier stage to see the information power and uniqueness<br>
```{r}
categorical_vars <- c("term", "grade", "sub_grade", "emp_title", "home_ownership", "verification_status", "loan_status", "purpose", "zip_code", "addr_state", "application_type", "policy_code")

meta_loans %>% select(variable, p_zeros, p_na, type, unique) %>% filter_(~ variable %in% categorical_vars) %>% knitr::kable()
```
Emp_title have too many unique values to be suitable for a classical categorical graph.

  2.2/Relationship between Default rate, Grade, Interest rate, Loan Amount and Income.
  
    a/ Loan Amount Vs. Grades
    
#Investigate the distribution of loan amount over the different grades<br> 
```{r}
give_count <- 
  stat_summary(fun.data = function(x) return(c(y = median(x)*1.06,
                                               label = length(x))),
               geom = "text")

give_mean <- 
  stat_summary(fun.y = mean, colour = "darkgreen", geom = "point", 
               shape = 18, size = 3, show.legend = FALSE)

train %>%
  ggplot(aes(grade, loan_amnt)) +
  geom_boxplot(fill = "white", colour = "darkblue", 
               outlier.colour = "red", outlier.shape = 1) +
  give_count +
  give_mean +
  scale_y_continuous(labels = comma) +
  facet_wrap(~ default) +
  labs(title="Loan Amount by Grade", x = "Grade", y = "Loan Amount \n")
```
We can derive a few points from the plot: there is not a lot of difference between default and non-default, lower quality loans tend to have a higher loan amount,
there are no outliers except for grade E and the loan amount spread (IQR) seems to be slightly higher for lower quality loans

    b/ Default rate Vs Grade
    
#Checking number of default loans in each grade level<br>
```{r}
g1 = train %>% filter(default == "TRUE") %>% group_by(grade) %>% summarise(default_count = n())

g1
```
#Creating default rate for each grade<br>
```{r}
g2 = train %>% group_by(grade) %>% summarise(count = n())
g3 <- g2 %>% left_join(g1) %>% mutate(default_rate = 100*default_count/count) %>% select(grade,count,default_count,default_rate)
g3
```
#Plot Default rate Vs Grade<br>
```{r}
ggplot(g3, aes(x=grade, y=default_rate, fill=grade)) + geom_bar(stat="identity")
```
As we would expect, riskier loans have higher default rates, except grade D has higher default rate than E, F and G. It is because the number of loans that got funded by the Lending Club for grade E, F and G are much smaller than other grades.

    c/ Loan Amount Vs Income
#Reshape the data to have both loan values plotted against loan amount<br>
```{r}
funded_amnt <- train %>% transmute(loan_amnt = loan_amnt, value = funded_amnt, variable = "funded_amnt")

funded_amnt_inv <- train %>% transmute(loan_amnt = loan_amnt, value = funded_amnt_inv, variable = "funded_amnt_inv")

plot_data <- rbind(funded_amnt, funded_amnt_inv)

rm(list = ls()[grep("^funded", ls())])
```
#Plot loan_amnt, funded_amnt, funded_amnt_inv<br>
```{r}
plot_data %>% ggplot(aes(x = loan_amnt, y = value)) + facet_wrap(~ variable, scales = "free_x", ncol = 3) + geom_point()
```
We can derive a few points from the plot: there are instances when funded amount is smaller loan amount, there seems to be a number of loans where investment is smaller funded amount i.e. not the full loan is invested in

#Annual income vs. loan amount only<br>
```{r}
train %>% ggplot(aes(x = annual_inc, y = loan_amnt)) + geom_point()
```
We can derive a few points from the plot: there is no immediatly discernible relationship, there are quite a few income outliers with questionable values (e.g. why would a person with annual income of 1 millione request a loan amount of 40,000)

###Part 3 (Machine Learning_Modeling) <br>

  1/Preparing data for modeling
  
#Creating a train_down dataset which contains an equal number of Default (TRUE) and non-default (FALSE) loans. Then we use the table() function to check that the downsampling is done correctly<br>
```{r}
train_down <- caret::downSample(x = train[, !(names(train) %in% c("default"))], y = as.factor(train$default), yname = "default")

base::prop.table(table(train_down$default))
```
#Proper names for character variables<br>
```{r}
vars_to_mutate <- train_down %>% select(which(sapply(.,is.character))) %>% names()

vars_to_mutate

train_down <- train_down %>% mutate_at(.funs = make.names, .vars = vars_to_mutate)
  
test <- test %>% mutate_at(.funs = make.names, .vars = vars_to_mutate)
```
#Dummy Variables<br>
```{r}
dummies_train <- dummyVars("~.", data = train_down[, !(names(train_down) %in% c("default"))], fullRank = FALSE)

train_down_dummy <- train_down %>% select(-which(sapply(.,is.character))) %>% cbind(predict(dummies_train, newdata = train_down))

dummies_test <- dummyVars("~.", data = test[, dummies_train$vars], fullRank = FALSE)

test_dummy <- test %>% select(one_of(colnames(train_down))) %>% select(-which(sapply(.,is.character))) %>% cbind(predict(dummies_test, newdata = test))
```
  
  2/ Logistic regression
  
#Building the model<br>
```{r}
levels(train_down$default)

model_glm_1 <- glm(formula = default ~ grade, family = binomial(link = "logit"), data = train_down, na.action = na.exclude)

class(model_glm_1)
```
#Summary of model_glm_1<br>
```{r}
summary(model_glm_1)
```
#Summary of coef for model_glm_1<br>
```{r}
summary(model_glm_1)$coef
```
#Model evaluation illustration<br>
```{r}
library(caret)

model_glm_1_pred <- predict.glm(object = model_glm_1, newdata = test, type = "response")

model_pred_t <- function(pred, t) ifelse(pred > t, TRUE, FALSE)

confusionMatrix(data = as.factor(model_pred_t(model_glm_1_pred, 0.5)), reference = as.factor(test$default), positive = "TRUE")
```
Looking at the model statistics, we find a mixed picture: we get a fair amount of true defaults right (sensitivity), we get a large amount of non-defaults wrong (specificity), the Kappa (which should consider class distributions) is very low

#ROC for glm_1<br>
```{r}
roc_glm_1 <- pROC::roc(response = test$default, predictor = model_glm_1_pred)

roc_glm_1
```
We see an area of 0.6311 which is better than random guessing but not too good.

#Plot ROC for roc_glm_1<br>
```{r}
pROC::plot.roc(x = roc_glm_1, legacy.axes = FALSE, xlim = c(1, 0), asp = NA, col = "green", print.auc = FALSE, print.auc.y = .4)

legend(x = "bottomright", legend=c("glm_1 AUC = 0.6311"), col = c("red"), lty = 1, cex = 1.0)
```
We see that the curve lies over the diagonal but it does not have a strong tendency to touch the upper left corner. A more complex model may perform better but would involve a larger amount of predictors 

  3/ Generalized Linear Model 
  
#Check for variables left for modeling<br>
```{r}
full_vars <- colnames(train_down)

full_vars
```
#Remove more variable<br>
```{r}
model_vars <- c("term", "grade", "emp_length", "annual_inc", "purpose", "default")

ignored_vars <- dplyr::setdiff(full_vars, model_vars)

ignored_vars
```
#Control data<br>
```{r}
ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 5, classProbs = TRUE, summaryFunction = twoClassSummary, savePredictions = TRUE, verboseIter = FALSE)
```
#Finishing train Dataset and creating model_glm_2 <br>
```{r}
model_glm_2 <- train_down %>% select(model_vars) %>% mutate(default = as.factor(ifelse(default == TRUE, "yes", "no"))) %>% filter(complete.cases(.)) %>%
train(default ~ ., data = ., method = "glm", family = "binomial", metric = "ROC", trControl = ctrl)

model_glm_2
```
#Summary of Model_glm_2<br>
```{r}
summary(model_glm_2)
```
#Predictor of model_glm_2<br>  
```{r}
predictors(model_glm_2)
```
#Check importance variables<br>
```{r}
varImp(model_glm_2)
```
#Plot Important Variables<br>
```{r}
plot(varImp(model_glm_2))
```
#Prediction model_glm_2<br>
```{r}
model_glm_2_pred <- predict(model_glm_2, newdata = test, type = "prob")

head(model_glm_2_pred, 3)
```
#Check performance of new model<br>
```{r}
caret::confusionMatrix( data = as.factor(ifelse(model_glm_2_pred[, "yes"] > 0.5, "yes", "no")), reference = as.factor(ifelse(test[complete.cases(test[, model_vars]), "default"] == TRUE, "yes", "no")))
```
#Build ROC for glme_2<br>
```{r}
temp <- as.factor(ifelse(test[complete.cases(test[, model_vars]), "default"] == TRUE, "yes", "no"))

roc_glm_2 <- pROC::roc(response = temp, predictor = model_glm_2_pred[, "yes"])

roc_glm_2
```
#Plot ROC roc_glm_1 and roc_glm_2<br>
```{r}
pROC::plot.roc(x = roc_glm_1, legacy.axes = FALSE, xlim = c(1, 0), asp = NA, 
               col = "green")

pROC::plot.roc(x = roc_glm_2, legacy.axes = FALSE, xlim = c(1, 0), asp = NA,
               add = TRUE, col = "blue")

legend(x = "bottomright", legend=c("glm_1 AUC = 0.631", "glm_2 AUC = 0.639"), 
       col = c("green", "blue"), lty = 1, cex = 1.0)
```

  4/ Trees Model
  
#Define control and train functions<br>
```{r}
ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 5, classProbs = TRUE, summaryFunction = twoClassSummary, verboseIter = FALSE, allowParallel = TRUE)

library(rpart)

model_rpart <- train_down %>% select(model_vars) %>% mutate(default = as.factor(ifelse(default == TRUE, "yes", "no"))) %>% filter(complete.cases(.)) %>% train(default ~ ., data = ., method = 'rpart', metric = "ROC", preProc = c("center", "scale"), trControl = ctrl)

model_rpart
```
#Plot Model_rpart<br>
```{r}
ggplot(model_rpart)
```
#The final tree<br>
```{r}
plot(model_rpart$finalModel, uniform = TRUE, margin = 0.2)
graphics::text(model_rpart$finalModel, cex = 0.3)
```
#Using Random Search<br>
```{r}
ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 5, classProbs = TRUE, summaryFunction = twoClassSummary, verboseIter = FALSE, allowParallel = TRUE, search = "random")

model_rpart <- train_down %>% select(model_vars) %>% mutate(default = as.factor(ifelse(default == TRUE, "yes", "no"))) %>% filter(complete.cases(.)) %>% train(default ~ ., data = ., method = 'rpart', metric = "ROC", preProc = c("center", "scale"), trControl = ctrl)

model_rpart
```
#Predict for model_rpart<br>
```{r}
model_rpart_pred <- predict(model_rpart, newdata = test %>% select(model_vars) %>% mutate(default = as.factor(ifelse(default == TRUE, "yes", "no"))) %>% filter(complete.cases(.)), type = "prob")

caret::confusionMatrix( data = as.factor(ifelse(model_rpart_pred[, "yes"] > 0.5, "yes", "no")), reference = as.factor(ifelse(test[complete.cases(test[,model_vars]), "default"] == TRUE, "yes", "no")))
```
#Compute ROC for roc_rpart<br>
```{r}
roc_rpart <- 
  pROC::roc(response = temp, 
            predictor = model_rpart_pred[, "yes"])

roc_rpart
```
The ROCAUC is 60% which is little lower comparing with cross-validated training set.

#Plot ROC for rpart comparing it against the ROC from earlier models<br>
```{r}
pROC::plot.roc(x = roc_glm_1, legacy.axes = FALSE, xlim = c(1, 0), asp = NA,
               col = "green")

pROC::plot.roc(x = roc_glm_2, legacy.axes = FALSE, xlim = c(1, 0), asp = NA,
               add = TRUE, col = "blue")

pROC::plot.roc(x = roc_rpart, legacy.axes = FALSE, xlim = c(1, 0), asp = NA,
               add = TRUE, col = "orange")


legend(x = "bottomright", legend=c("glm_1 AUC = 0.631", "glm_2 AUC = 0.639",
                                   "rpart AUC = 0.625"), 
       col = c("green", "blue", "orange"), lty = 1, cex = 1.0)
```
 
 5/ Random Forest
 
#Creating data and model for Random Forest<br>
```{r}
library(randomForest)

ctrl <- trainControl(method = "repeatedcv", number = 5, repeats = 1, classProbs = TRUE, summaryFunction = twoClassSummary, verboseIter = FALSE, allowParallel = TRUE)

model_rf <- train_down %>% select(model_vars) %>% mutate(default = as.factor(ifelse(default == TRUE, "yes", "no"))) %>% filter(complete.cases(.)) %>% train(default ~ ., data = ., method = 'rf', ntree = 10, metric = "ROC", preProc = c("center", "scale"), trControl = ctrl)

model_rf
```
#Plot model_rf<br>
```{r}
plot(model_rf$finalModel)
```
#Prediction and ConfusionMatrix for model_rf<br>
```{r}
model_rf_pred <- predict(model_rf, newdata = test %>% select(model_vars) %>% mutate(default = as.factor(ifelse(default == TRUE, "yes", "no"))) %>% filter(complete.cases(.)), type = "prob")

caret::confusionMatrix( data = as.factor(ifelse(model_rf_pred[, "yes"] > 0.5, "yes", "no")), reference = as.factor(ifelse(test[complete.cases(test[,model_vars]), "default"] == TRUE, "yes", "no")))
```
#Compute ROC for roc_rf<br>
```{r}
roc_rf <- pROC::roc(response = temp, predictor = model_rf_pred[, "yes"])

roc_rf
```
#Plot ROC for rf comparing it against the ROC from earlier models<br>
```{r}
pROC::plot.roc(x = roc_glm_1, legacy.axes = FALSE, xlim = c(1, 0), asp = NA,
col = "green")

pROC::plot.roc(x = roc_glm_2, legacy.axes = FALSE, xlim = c(1, 0), asp = NA,
add = TRUE, col = "blue")

pROC::plot.roc(x = roc_rpart, legacy.axes = FALSE, xlim = c(1, 0), asp = NA,
add = TRUE, col = "orange")

pROC::plot.roc(x = roc_rf, legacy.axes = FALSE, xlim = c(1, 0), asp = NA,
add = TRUE, col = "red")

legend(x = "bottomright", legend=c("glm_1 AUC = 0.631", "glm_2 AUC = 0.639",
"rpart AUC = 0.625", "rf AUC= 0.594"), col = c("green", "blue", "orange", "red"), lty = 1, cex = 1.0)
```

  6/ Stochastic Gradient Boosting
  
#Build dataset for the model<br>
```{r}
library(gbm)

ctrl <- trainControl(method = "repeatedcv", number = 5, repeats = 1, classProbs = TRUE, summaryFunction = twoClassSummary, verboseIter = FALSE, allowParallel = TRUE)

model_gbm_1 <- train_down %>% select(model_vars) %>% mutate(default = as.factor(ifelse(default == TRUE, "yes", "no"))) %>% filter(complete.cases(.)) %>%
train(default ~ ., data = ., method = "gbm", metric = "ROC", trControl = ctrl, preProc = c("center", "scale"), verbose = FALSE)

model_gbm_1
```
#Plotting the boosting iterations<br>
```{r}
ggplot(model_gbm_1)
```
#Prediction and confusionmatric for model_gbm_1<br>
```{r}
model_gbm_1_pred <- predict(model_gbm_1, newdata = test %>% select(model_vars) %>%
mutate(default = as.factor(ifelse(default == TRUE, "yes", "no"))) %>% filter(complete.cases(.)), type = "prob")

caret::confusionMatrix( data = as.factor(ifelse(model_gbm_1_pred[, "yes"] > 0.5, "yes", "no")), reference = as.factor(ifelse(test[complete.cases(test[,model_vars]), "default"] == TRUE, "yes", "no")))
```
#ROC for gbm_1<br>
```{r}
roc_gbm_1 <- pROC::roc(response = temp, predictor = model_gbm_1_pred[, "yes"])

roc_gbm_1
```
#The ROCAUC is ~6372% which is nearly the same as for cross-validated training set. We plot the curve comparing it against the ROC from other models<br>
```{r}
pROC::plot.roc(x = roc_glm_1, legacy.axes = FALSE, xlim = c(1, 0), asp = NA,
col = "green")

pROC::plot.roc(x = roc_glm_2, legacy.axes = FALSE, xlim = c(1, 0), asp = NA,
add = TRUE, col = "blue")

pROC::plot.roc(x = roc_rpart, legacy.axes = FALSE, xlim = c(1, 0), asp = NA,
add = TRUE, col = "orange")

pROC::plot.roc(x = roc_gbm_1, legacy.axes = FALSE, xlim = c(1, 0), asp = NA,
add = TRUE, col = "brown")

legend(x = "bottomright", legend=c("glm_1 AUC = 0.631", "glm_2 AUC = 0.639", "rpart AUC = 0.625", "gbm AUC = 0.637"), col = c("green", "blue", "orange", "brown"), lty = 1, cex = 1.0)
```

Conclusion:
After carefully analyzed the data,
Based on the models: Logistic regression, Linear Model , Decision Tree, Random Forest and Stochastic Gradient Boosting model,Kappa statistics from all models are around 20%, which indicated that they perform moderately better than chance. The AUC also almost the same for models, around 0.63.

With Kappa statistics and AUC are almost the same for all model, the Random Forest model does provide a better performance the accuracy is at 62% comparing to other models which yields around 50% accuracy. 

